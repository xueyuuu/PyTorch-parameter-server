Pytorch ps:


pre-requise:
pytorch
pip3 install torch torchvision

pip3 install pandas
pip3 install blosc

error: Cannot compile MPI programs. Check your configuration!!!

sudo apt install libopenmpi-dev
pip3 install mpi4py


for each in $(kubectl get pods -n ps | awk '{print $1}');
do
  kubectl exec -it $each -- /usr/sbin/service ssh start
done



cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'scp -r 10.244.29.16:~/hosts /etc/hosts'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'scp -r master:~/PyTorch-parameter-server/tools/hosts_address ~/PyTorch-parameter-server/tools/hosts_addres'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'sed -i 's/python/python3/g' PyTorch-parameter-server/src/data_prepare.sh'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'cd ~/PyTorch-parameter-server/src && ./data_prepare.sh'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'cd ~/PyTorch-parameter-server && git pull'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'cd ~/PyTorch-parameter-server && git config --global --unset http.proxy && git config --global --unset https.proxy'

cat ~/PyTorch-parameter-server/tools/hosts_address | xargs -I{} ssh -o StrictHostKeyChecking=no {} 'chmod +x ./PyTorch-parameter-server/src/run_pytorch_dist.sh ./PyTorch-parameter-server/src/launch.sh'

git config --global --unset http.proxy
git config --global --unset https.proxy


 UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.



ssh worker2 "cd ${HOME}/PyTorch-parameter-server/src; nohup bash ${HOME}/PyTorch-parameter-server/src/run_pytorch_dist.sh \"1\" \"4\" \"10.244.29.16\" & >/dev/null &



nohup ${HOME}/PyTorch-parameter-server/src/run_pytorch_dist.sh 0 4 10.244.29.16 &

NODE_RANK="$1"
NNODE="$2"
MASTER_IP="$3"
SRC_DIR=${HOME}/PyTorch-parameter-server/src

echo ${MASTER_IP}
python3 -m torch.distributed.launch \
--nproc_per_node=1 \
--nnodes=${NNODE} --node_rank=${NODE_RANK} --master_addr="${MASTER_IP}" --master_port=1234 \
${SRC_DIR}/distributed_nn.py \
--lr=0.1 \
--momentum=0.9 \
--max-steps=100000 \
--epochs=100 \
--network=ResNet18 \
--dataset=Cifar10 \
--batch-size=64 \
--comm-type=Bcast \
--num-aggregate=2 \
--mode=normal \
--eval-freq=2000 \
--gather-type=gather \
--compress-grad=compress \
--enable-gpu= \
--train-dir=/home/hduser > out_node_${NODE_RANK} 2>&1



ssh 10.244.29.16 "cd $${HOME}/PyTorch-parameter-server/src && nohup bash ${HOME}/PyTorch-parameter-server/src/run_pytorch_dist.sh \"$((${i}-1))\" \"4\" \"10.244.29.16\" &>/dev/null &"


sudo scp -r hduser@10.244.29.16:~/hosts /etc/hosts
10.244.29.16 fl-6787598b4-b5ln7
10.244.12.10 fl-6787598b4-cmvv4
10.244.31.15 fl-6787598b4-d5lph
10.244.11.10 fl-6787598b4-vg2kd